{"ast":null,"code":"import _regeneratorRuntime from \"/Users/shouryagupta/Desktop/MS Engage 21/MSEngage21_VC/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"/Users/shouryagupta/Desktop/MS Engage 21/MSEngage21_VC/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\n\nexport class Optimizer extends Serializable {\n  /**\n   * Executes `f()` and minimizes the scalar output of `f()` by computing\n   * gradients of y with respect to the list of trainable variables provided by\n   * `varList`. If no list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to minimize.\n   * @param returnCost Whether to return the scalar cost value produced by\n   * executing `f()`.\n   * @param varList An optional list of variables to update. If specified, only\n   * the trainable variables in varList will be updated by minimize. Defaults to\n   * all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  minimize(f, returnCost = false, varList) {\n    const _this$computeGradient = this.computeGradients(f, varList),\n          value = _this$computeGradient.value,\n          grads = _this$computeGradient.grads;\n\n    if (varList != null) {\n      const gradArray = varList.map(v => ({\n        name: v.name,\n        tensor: grads[v.name]\n      }));\n      this.applyGradients(gradArray);\n    } else {\n      this.applyGradients(grads);\n    } // Dispose gradients.\n\n\n    dispose(grads);\n\n    if (returnCost) {\n      return value;\n    } else {\n      value.dispose();\n      return null;\n    }\n  }\n  /**\n   * The number of iterations that this optimizer instance has been invoked for.\n   */\n\n\n  get iterations() {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n\n    return this.iterations_;\n  }\n\n  incrementIterations() {\n    this.iterations_ = this.iterations + 1;\n  }\n  /**\n   * Executes f() and computes the gradient of the scalar output of f() with\n   * respect to the list of trainable variables provided by `varList`. If no\n   * list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to use for computing\n   * gradients with respect to variables.\n   * @param varList An optional list of variables to compute gradients with\n   * respect to. If specified, only the trainable variables in varList will have\n   * gradients computed with respect to. Defaults to all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n\n\n  computeGradients(f, varList) {\n    return variableGrads(f, varList);\n  }\n  /**\n   * Dispose the variables (if any) owned by this optimizer instance.\n   */\n\n\n  dispose() {\n    if (this.iterations_ != null) {\n      dispose(this.iterations_);\n    }\n  }\n\n  saveIterations() {\n    var _this = this;\n\n    return _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee() {\n      return _regeneratorRuntime.wrap(function _callee$(_context) {\n        while (1) switch (_context.prev = _context.next) {\n          case 0:\n            if (_this.iterations_ == null) {\n              _this.iterations_ = 0;\n            }\n\n            return _context.abrupt(\"return\", {\n              name: 'iter',\n              // TODO(cais): Use 'int64' type when available.\n              tensor: scalar(_this.iterations_, 'int32')\n            });\n\n          case 2:\n          case \"end\":\n            return _context.stop();\n        }\n      }, _callee);\n    }))();\n  }\n\n  getWeights() {\n    return _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2() {\n      return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n        while (1) switch (_context2.prev = _context2.next) {\n          case 0:\n            throw new Error('getWeights() is not implemented for this optimizer yet.');\n\n          case 1:\n          case \"end\":\n            return _context2.stop();\n        }\n      }, _callee2);\n    }))();\n  }\n\n  setWeights(weightValues) {\n    var _this2 = this;\n\n    return _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3() {\n      return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n        while (1) switch (_context3.prev = _context3.next) {\n          case 0:\n            throw new Error(\"setWeights() is not implemented for this optimizer class \" + \"\".concat(_this2.getClassName()));\n\n          case 1:\n          case \"end\":\n            return _context3.stop();\n        }\n      }, _callee3);\n    }))();\n  }\n  /**\n   * Extract the first element of the weight values and set it\n   * as the iterations counter variable of this instance of optimizer.\n   *\n   * @param weightValues\n   * @returns Weight values with the first element consumed and excluded.\n   */\n\n\n  extractIterations(weightValues) {\n    var _this3 = this;\n\n    return _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4() {\n      return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n        while (1) switch (_context4.prev = _context4.next) {\n          case 0:\n            _context4.next = 2;\n            return weightValues[0].tensor.data();\n\n          case 2:\n            _this3.iterations_ = _context4.sent[0];\n            return _context4.abrupt(\"return\", weightValues.slice(1));\n\n          case 4:\n          case \"end\":\n            return _context4.stop();\n        }\n      }, _callee4);\n    }))();\n  }\n\n}\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: instance => {\n    return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;\n  }\n});","map":{"version":3,"sources":["../../src/optimizers/optimizer.ts"],"names":[],"mappings":";;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,OAAR,QAAsB,YAAtB;AACA,SAAQ,aAAR,QAA4B,cAA5B;AACA,SAAQ,MAAR,QAAqB,YAArB;AACA,SAAQ,YAAR,QAA2B,kBAA3B;AAoBA;;AACA,OAAM,MAAgB,SAAhB,SAAkC,YAAlC,CAA8C;AAGlD;;;;;;;;;;;;;AAaG;AACH,EAAA,QAAQ,CAAC,CAAD,EAAkB,UAAU,GAAG,KAA/B,EAAsC,OAAtC,EAA0D;AAAA,kCAEzC,KAAK,gBAAL,CAAsB,CAAtB,EAAyB,OAAzB,CAFyC;AAAA,UAEzD,KAFyD,yBAEzD,KAFyD;AAAA,UAElD,KAFkD,yBAElD,KAFkD;;AAIhE,QAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,YAAM,SAAS,GACX,OAAO,CAAC,GAAR,CAAY,CAAC,KAAK;AAAC,QAAA,IAAI,EAAE,CAAC,CAAC,IAAT;AAAe,QAAA,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,IAAH;AAA5B,OAAL,CAAb,CADJ;AAEA,WAAK,cAAL,CAAoB,SAApB;AACD,KAJD,MAIO;AACL,WAAK,cAAL,CAAoB,KAApB;AACD,KAV+D,CAYhE;;;AACA,IAAA,OAAO,CAAC,KAAD,CAAP;;AAEA,QAAI,UAAJ,EAAgB;AACd,aAAO,KAAP;AACD,KAFD,MAEO;AACL,MAAA,KAAK,CAAC,OAAN;AACA,aAAO,IAAP;AACD;AACF;AAED;;AAEG;;;AACH,MAAI,UAAJ,GAAc;AACZ,QAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,WAAK,WAAL,GAAmB,CAAnB;AACD;;AACD,WAAO,KAAK,WAAZ;AACD;;AAES,EAAA,mBAAmB,GAAA;AAC3B,SAAK,WAAL,GAAmB,KAAK,UAAL,GAAkB,CAArC;AACD;AAED;;;;;;;;;;;;AAYG;;;AACH,EAAA,gBAAgB,CAAC,CAAD,EAAkB,OAAlB,EAAsC;AAEpD,WAAO,aAAa,CAAC,CAAD,EAAI,OAAJ,CAApB;AACD;AAYD;;AAEG;;;AACH,EAAA,OAAO,GAAA;AACL,QAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,MAAA,OAAO,CAAC,KAAK,WAAN,CAAP;AACD;AACF;;AAEK,EAAA,cAAN,GAAoB;AAAA;;AAAA;AAAA;AAAA;AAAA;AAClB,gBAAI,KAAI,CAAC,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,cAAA,KAAI,CAAC,WAAL,GAAmB,CAAnB;AACD;;AAHiB,6CAIX;AACL,cAAA,IAAI,EAAE,MADD;AAEL;AACA,cAAA,MAAM,EAAE,MAAM,CAAC,KAAI,CAAC,WAAN,EAAmB,OAAnB;AAHT,aAJW;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASnB;;AAEK,EAAA,UAAN,GAAgB;AAAA;AAAA;AAAA;AAAA;AAAA,kBACR,IAAI,KAAJ,CAAU,yDAAV,CADQ;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEf;;AAEK,EAAA,UAAN,CAAiB,YAAjB,EAA4C;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA,kBACpC,IAAI,KAAJ,CACF,wEACG,MAAI,CAAC,YAAL,EADH,CADE,CADoC;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAI3C;AAED;;;;;;AAMG;;;AACa,EAAA,iBAAN,CAAwB,YAAxB,EAAmD;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBAEjC,YAAY,CAAC,CAAD,CAAZ,CAAgB,MAAhB,CAAuB,IAAvB,EAFiC;;AAAA;AAE3D,YAAA,MAAI,CAAC,WAFsD,kBAEF,CAFE;AAAA,8CAGpD,YAAY,CAAC,KAAb,CAAmB,CAAnB,CAHoD;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAI5D;;AA3HiD;AA8HpD,MAAM,CAAC,cAAP,CAAsB,SAAtB,EAAiC,MAAM,CAAC,WAAxC,EAAqD;AACnD,EAAA,KAAK,EAAG,QAAD,IAAwB;AAC7B,WAAO,QAAQ,CAAC,QAAT,IAAqB,IAArB,IAA6B,QAAQ,CAAC,gBAAT,IAA6B,IAA1D,IACH,QAAQ,CAAC,cAAT,IAA2B,IAD/B;AAED;AAJkD,CAArD","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {dispose} from '../globals';\nimport {variableGrads} from '../gradients';\nimport {scalar} from '../ops/ops';\nimport {Serializable} from '../serialization';\nimport {Scalar, Variable} from '../tensor';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\n/**\n * A variable that belongs to an optimizer.\n *\n * The `originalName` field is required for keeping track of the canonical\n * name of the variable, which is usually the name of the model weight that\n * the variable is related to plus a suffix, e.g., 'dense1/kernel/momentum'.\n * The name of the `Variable` object itself cannot be used directly due to\n * possible deduplication: Every `Variable` must have a unique name but more\n * than one optimizer objects of the same type may be created for the same model\n * or the same `Variable`.\n */\nexport interface OptimizerVariable {\n  originalName: string;\n  variable: Variable;\n}\n\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\nexport abstract class Optimizer extends Serializable {\n  protected iterations_: number;\n\n  /**\n   * Executes `f()` and minimizes the scalar output of `f()` by computing\n   * gradients of y with respect to the list of trainable variables provided by\n   * `varList`. If no list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to minimize.\n   * @param returnCost Whether to return the scalar cost value produced by\n   * executing `f()`.\n   * @param varList An optional list of variables to update. If specified, only\n   * the trainable variables in varList will be updated by minimize. Defaults to\n   * all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  minimize(f: () => Scalar, returnCost = false, varList?: Variable[]): Scalar\n      |null {\n    const {value, grads} = this.computeGradients(f, varList);\n\n    if (varList != null) {\n      const gradArray: NamedTensor[] =\n          varList.map(v => ({name: v.name, tensor: grads[v.name]}));\n      this.applyGradients(gradArray);\n    } else {\n      this.applyGradients(grads);\n    }\n\n    // Dispose gradients.\n    dispose(grads);\n\n    if (returnCost) {\n      return value;\n    } else {\n      value.dispose();\n      return null;\n    }\n  }\n\n  /**\n   * The number of iterations that this optimizer instance has been invoked for.\n   */\n  get iterations(): number {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n    return this.iterations_;\n  }\n\n  protected incrementIterations() {\n    this.iterations_ = this.iterations + 1;\n  }\n\n  /**\n   * Executes f() and computes the gradient of the scalar output of f() with\n   * respect to the list of trainable variables provided by `varList`. If no\n   * list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to use for computing\n   * gradients with respect to variables.\n   * @param varList An optional list of variables to compute gradients with\n   * respect to. If specified, only the trainable variables in varList will have\n   * gradients computed with respect to. Defaults to all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  computeGradients(f: () => Scalar, varList?: Variable[]):\n      {value: Scalar, grads: NamedTensorMap} {\n    return variableGrads(f, varList);\n  }\n\n  /**\n   * Updates variables by using the computed gradients.\n   *\n   * @param variableGradients A mapping of variable name to its gradient value.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  abstract applyGradients(variableGradients: NamedTensorMap|\n                          NamedTensor[]): void;\n\n  /**\n   * Dispose the variables (if any) owned by this optimizer instance.\n   */\n  dispose(): void {\n    if (this.iterations_ != null) {\n      dispose(this.iterations_);\n    }\n  }\n\n  async saveIterations(): Promise<NamedTensor> {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n    return {\n      name: 'iter',  // Named for Python compatibility.\n      // TODO(cais): Use 'int64' type when available.\n      tensor: scalar(this.iterations_, 'int32')\n    };\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    throw new Error('getWeights() is not implemented for this optimizer yet.');\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    throw new Error(\n        `setWeights() is not implemented for this optimizer class ` +\n        `${this.getClassName()}`);\n  }\n\n  /**\n   * Extract the first element of the weight values and set it\n   * as the iterations counter variable of this instance of optimizer.\n   *\n   * @param weightValues\n   * @returns Weight values with the first element consumed and excluded.\n   */\n  protected async extractIterations(weightValues: NamedTensor[]):\n      Promise<NamedTensor[]> {\n    this.iterations_ = (await weightValues[0].tensor.data())[0];\n    return weightValues.slice(1);\n  }\n}\n\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: (instance: Optimizer) => {\n    return instance.minimize != null && instance.computeGradients != null &&\n        instance.applyGradients != null;\n  }\n});\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}